# CLIP: Learning Transferable Visual Models From Natural Language Supervision(2021)

> - Authors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever
> - Created: 2023년 7월 8일 오후 2:38
> - Published: ICML 2021
> - Tags: Vision-Language, Multimodal

## Summary
- 자연어 supervision으로 image representation learning은 좋지 않은 성능을 보이는데, zero-shot learning에서도 현저히 낮은 정확도를 보임
  - weakly supervised model은 어느 정도 개선은 있었지만, 모델의 유연성을 단축시키고 zero-shot 능력을 제한함
- 본 논문은 weakly supervised model과 text를 통한 image representation의 학습에서 가장 중요한 차이는 데이터 규모로 보았으며, 이 차이를 줄이기 위해 400M개 쌍의 (이미지, 텍스트) 데이터를 수집함
- 또한, Contrastive Learning을 통해, 대규모의 text 데이터를 학습하고 이를 image와 연결해주어, 다양한 vision task에서 좋은 결과를 보여줌

## Approach
1. Natural Language Supervision
    - 이미지분류에서 기존 crowd-sourced labeling과 비교했을 때, 자연어 supervision은 ‘ML compatible format’을 따르지 않기 때문에 확장하기 더욱 쉬움
    - 자연어로부터 학습하는 것은 representation을 ‘그냥’ 학습하지 않고, 언어에 대한 representation까지도 연결하기 때문에, zero-shot transfer를 가능하게 함
2. Creating a Sufficiently Large Dataset
    - 기존의 연구는 주로 MS-COCO, Visual Genome, YFCC100M 등의 데이터를 사용하였지만, CLIP은 인터넷에 공개된 방대한 데이터셋을 수집하여 400M개의 이미지와 텍스트 쌍으로 구성된 데이터를 사용함

## Method
- selected approach: bag-of-words contrastive 방식
- Contrastive Learning은 두 개의 input에 대해 각각 encoder 통과 후 나온 두 embedding 간의 유사도를 계산하는 방식
- CLIP의 Method는 다음과 같음
    - batch마다 N개의 (image, text) 쌍이 존재할 때, 각각 encoder를 통과해 나온 N개의 embedding 결과의 유사도 계산
    - positive pair(diagonal)에서의 코사인 유사도는 최대화하고, negative pair(off-diagonal)에서의 유사도는 최소화하도록 loss 설정하여 학습
- 따라서 임의의 이미지가 입력되었을 때, image와 text embedding의 유사도를 비교하여, 가장 높은 항목을 text label로 선정됨

    → 학습에 사용되지 않은 데이터도 prediction 가능 (zero-shot learning)