# LightHam: Is Attention Better Than Matrix Decompostion?(2021)

> - Authors: Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens
> - Created: 2023년 04월 19일 오후 2:34
> - Published: ICLR 2021
> - Tags: Attention, Matrix Decompostion

## Summary

- 가설: global context에서 hand-crafted attention이 대체될 수 있는가?
- 딥러닝에서 어텐션 메커니즘, 그중에서도 self-attention이 중요한 역할을 하고 있다.
- 20년 전 개발된 MD(matrix decomposition) 모델이 self-attention보다 성능과 장기 의존성을 인코딩하는 과정에서 발생하는 계산비용적인 측면에서 더 낫다는 흥미로운 발견을 했다.
- 따라서, 본 논문은 global context 모듈을 디자인하기 위해 새로운 접근법에 집중한다.
    - main idea: 우리가 inductive bias를 global context처럼 목적함수로 공식화한다면, 목적함수를 줄이기 위한 최적화 알고리즘을 우리가 네트워크에 필요한 아키텍처로 구성할 수 있다!
- 가장 대표적인 global context 모듈인 self-attention에 대응하는 모듈을 개발함으로써 이 아이디어를 구체화한다.
- 본 논문은 global correltation block인 “Hamburger”를 제안한다.
    - matrix decompostion을 해결하기 위한 반복 최적화 알고리즘은을Hamburger 아키텍처로 정의한다.
    - 네트워크 내 고유의 correlation을 찾아 global infromation을 추출하여 모델링하고, 이를 matrix decomposition을 통해 해결한다.
    - 깨끗한 하위신호 부분공간을 복구하기 위해 학습된 representation을 부분행렬로 분해해 사용한다.
- 벡터양자화, 컨셉분해 등 행렬분해 관련 모델에서 장점을 갖고, Back-Propagation Through Time 알고리즘 대신 trucated BPTT를 사용함으로써 1-step으로 gradient를 효율적으로 계산한다는 이점을 갖는다. 또한, 
**semantic segmentation과 image 생성을 포함한 근본적인 CV task에도 SOTA와 경쟁할만큼 좋은 결과를 보인다.**